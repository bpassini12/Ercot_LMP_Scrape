{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60402a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import datetime \n",
    "import yagmail\n",
    "from cryptography.fernet import Fernet\n",
    "import traceback\n",
    "import logging\n",
    "import bp_sql as bp\n",
    "import zipfile\n",
    "import time\n",
    "#import html5lib\n",
    "\n",
    "#for chrome driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(800, 800))  \n",
    "display.start()\n",
    "\n",
    "prefs = {\"download.default_directory\" : zip_fldr_path}\n",
    "options.add_experimental_option(\"prefs\",prefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b83561",
   "metadata": {},
   "source": [
    "## Set Up dates and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aa551",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_start_time = time.time()\n",
    "\n",
    "today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#DB path\n",
    "spp_db = 'SPP.db'\n",
    "\n",
    "zip_fldr = 'SPP_Zips'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#zip folder path\n",
    "zip_fldr_path = os.path.join(cwd,  zip_fldr)\n",
    "\n",
    "#contents of zip folder path\n",
    "zip_list = os.listdir(os.path.join(cwd, zip_fldr))\n",
    "\n",
    "#Website and file type to download\n",
    "Domain = 'http://ercot.com'\n",
    "url = 'https://www.ercot.com/mp/data-products/data-product-details?id=NP6-785-ER'\n",
    "\n",
    "mon_dict = {1:'JAN', 2:'FEB', 3:'MAR', 4:'APR', 5:'MAY', 6:'JUN', 7:'JUL', 8:'AUG', 9:'SEP', 10:'OCT', 11:'NOV', 12:'DEC'}\n",
    "revs_mon_dict = dict([(value, key) for key, value in mon_dict.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838497f2",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    '''read in gmail creds'''\n",
    "    key_path = os.path.join(os.path.expanduser('~'), '.fernet')\n",
    "    key = pickle.load(open(key_path, 'rb'))\n",
    "    cipher_suite = Fernet(key)\n",
    "    encrypted_credentials_df = pd.read_csv('encrypted_credentials.csv')\n",
    "\n",
    "    gmail = 'Gmail'\n",
    "    gmail_ec_row = encrypted_credentials_df.loc[encrypted_credentials_df.login_account == gmail]\n",
    "    gmail_user = gmail_ec_row.iloc[0]['username']\n",
    "    gmail_pwd_encrypt = gmail_ec_row.iloc[0]['encrypted_password']\n",
    "    gmail_pwd = cipher_suite.decrypt(str.encode(gmail_pwd_encrypt)).decode('utf-8')\n",
    "    \n",
    "    return gmail_user, gmail_pwd\n",
    "\n",
    "\n",
    "def send_email(email_subject, email_contents):\n",
    "    '''send email using gmail'''\n",
    "    gmail_user, gmail_pwd = get_credentials()\n",
    "    \n",
    "    yag = yagmail.SMTP(user=gmail_user, password=gmail_pwd)\n",
    "    \n",
    "    yag.send(to=gmail_user, subject=email_subject, contents=email_contents)\n",
    "\n",
    "def check_max_date(db_file):\n",
    "    '''check the spp db for the last date and record that was saved.\n",
    "       This will be used for a starting point when looking to pull new data'''\n",
    "\n",
    "    conn = bp.create_connection(db_file)\n",
    "    max_date_df = pd.read_sql_query('''Select max(DELIVERY_DATE) as MAX_DELIVERY_DATE,  max(DELIVERY_HOUR) as MAX_DELIVERY_HOUR \n",
    "                                       from ercot_hist_spp \n",
    "                                       where delivery_date = (select max(delivery_date) from ercot_hist_spp where settlement_point_price is not null)''', conn)\n",
    "    \n",
    "    max_date =  pd.to_datetime(max_date_df['MAX_DELIVERY_DATE'][0])\n",
    "    max_hour = max_date_df['MAX_DELIVERY_HOUR'][0]\n",
    "    conn.close()\n",
    "    return max_date, max_hour\n",
    "\n",
    "def get_sheet_list(file:str, year:int):\n",
    "    '''for each xlsx that needs to be looked at, get the list of sheets that need to be read in and compared\n",
    "       If reading in the same years xlsx as the max date from db, only get sheets as of that max date and later'''\n",
    "\n",
    "    with zipfile.ZipFile(file) as zipped_file:\n",
    "        summary = zipped_file.open(r'xl/workbook.xml').read()\n",
    "    soup = BeautifulSoup(summary, 'xml')\n",
    "    sheet_list = [sheet.get(\"name\") for sheet in soup.find_all(\"sheet\")]\n",
    "\n",
    "    if year == min_file_year:\n",
    "        sheets_list = [x for x in sheet_list if revs_mon_dict.get(str(x[0:3]).upper())>= min_file_mon]\n",
    "\n",
    "    return sheet_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a14d9",
   "metadata": {},
   "source": [
    "## Define SQL Strings \n",
    "- create view, tbl, and index if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38224d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_create_spp_table = ''' Create Table if not exists ercot_hist_spp (\n",
    "                                    DELIVERY_DATE text,\n",
    "                                    DELIVERY_HOUR integer,\n",
    "                                    DELIVERY_INTERVAL integer,\n",
    "                                    REPEATED_HOUR_FLAG text,\n",
    "                                    SETTLEMENT_POINT_NAME text,\n",
    "                                    SETTLEMENT_POINT_TYPE text,\n",
    "                                    SETTLEMENT_POINT_PRICE real);\n",
    "                                '''\n",
    "\n",
    "sql_create_spp_tbl_index = '''Create index IF NOT EXISTS index_dd_ercot_hist_spp on ercot_hist_spp (DELIVERY_DATE)'''\n",
    "\n",
    "sql_create_spp_view = ''' Create View if not exists ercot_avg_spp as\n",
    "                                Select DELIVERY_DATE, DELIVERY_HOUR, SETTLEMENT_POINT_NAME, SETTLEMENT_POINT_TYPE, AVG(SETTLEMENT_POINT_PRICE) as SETTLEMENT_POINT_PRICE \n",
    "                                from ercot_hist_spp \n",
    "                                group by DELIVERY_DATE, DELIVERY_HOUR, SETTLEMENT_POINT_NAME, SETTLEMENT_POINT_TYPE\n",
    "                                ;\n",
    "                                '''\n",
    "\n",
    "#create tbl and view if they dont exist\n",
    "create_list = [sql_create_spp_table, sql_create_spp_tbl_index, sql_create_spp_view]\n",
    "for c in create_list:\n",
    "    bp.create_table(spp_db, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f0b2a",
   "metadata": {},
   "source": [
    "## Run the Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #get max date of data in db\n",
    "    max_date, max_hour = check_max_date(spp_db)\n",
    "\n",
    "    if None not in (max_date, max_hour):\n",
    "        max_year = max_date.year\n",
    "        max_mon = max_date.month\n",
    "        max_day = max_date.day\n",
    "    else:\n",
    "        max_year, max_mon, max_day = 2000, 1 , 1\n",
    "        max_date = date(max_year, max_mon, max_day).strftime(\"%m/%d/%y\")\n",
    "        \n",
    "\n",
    "    max_mon_abrv = mon_dict.get(max_mon)\n",
    "\n",
    "    if max_mon == 12 and max_day == 31 and max_hour == 24:\n",
    "        min_file_year = max_year + 1\n",
    "        min_file_mon = 1\n",
    "    else:\n",
    "        min_file_year = max_year\n",
    "        min_file_mon = max_mon\n",
    "\n",
    "    #Get websites HTML, get all the filename and associated links\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    file_list = soup.find_all(class_='name')\n",
    "    friendly_list = [f.next_element for f in file_list]\n",
    "    long_name_list = [f['title'] for f in file_list]\n",
    "    link_list = soup.findAll('a', attrs={'href': re.compile(\"/misdownload/\")}) \n",
    "    link_list = [f['href'] for f in link_list]\n",
    "\n",
    "    website_file_df = pd.DataFrame(zip(friendly_list, long_name_list, link_list), columns=['web_friendly_name','web_long_name','web_link'])  \n",
    "\n",
    "\n",
    "    website_file_df['web_file_yr'] = website_file_df.web_friendly_name.apply(lambda x: int(x[-4:]))\n",
    "    website_file_df['web_file_date'] = website_file_df.web_long_name.apply(lambda x: pd.to_datetime(x.split('.')[3]))\n",
    "\n",
    "\n",
    "    zip_list_df = pd.DataFrame(zip_list, columns=['fldr_filename'])\n",
    "    zip_list_df['fdr_friendly_name'] = zip_list_df.fldr_filename.apply(lambda x: x.split('.')[3])\n",
    "\n",
    "    merge_df = website_file_df.merge(zip_list_df,how='outer',left_on='web_friendly_name', right_on='fdr_friendly_name')\n",
    "\n",
    "\n",
    "\n",
    "    #Download necessary zips\n",
    "    for i, r in merge_df[~merge_df.web_long_name.isna()].iterrows():\n",
    "\n",
    "        #download new zip\n",
    "        if (r['web_file_date']>max_date and r['web_file_yr']>= min_file_year):\n",
    "            with open(os.path.join(zip_fldr_path, r['web_long_name']), 'wb') as file:\n",
    "                response = requests.get(r['web_link'])\n",
    "                file.write(response.content)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    #extracts the xlsx from each zip and places in same directory\n",
    "    z_list = [j for j in os.listdir(zip_fldr_path) if '.zip' in j]\n",
    "\n",
    "    #removes zips after extracting excel\n",
    "    for file in z_list:\n",
    "        with zipfile.ZipFile(zip_fldr_path + '/' + file, 'r') as zipObj:\n",
    "            zipObj.extractall(path=zip_fldr_path)\n",
    "            os.remove(os.path.join(zip_fldr_path, file))\n",
    "\n",
    "    bp.delete_rows(spp_db, delete_sql='''delete from ercot_hist_spp where settlement_point_price is null''')\n",
    "\n",
    "    #create blank lists\n",
    "    file_list, month_list, start_date_list, end_date_list, time_list  = ([] for i in range(5))\n",
    "\n",
    "    column_dict = {'Delivery Date':'DELIVERY_DATE'\n",
    "                    , 'Delivery Hour':'DELIVERY_HOUR'\n",
    "                    ,'Delivery Interval':'DELIVERY_INTERVAL'\n",
    "                    ,'Repeated Hour Flag':'REPEATED_HOUR_FLAG'\n",
    "                    ,'Settlement Point Name':'SETTLEMENT_POINT_NAME'\n",
    "                    ,'Settlement Point Type':'SETTLEMENT_POINT_TYPE'\n",
    "                    ,'Settlement Point Price':'SETTLEMENT_POINT_PRICE'}\n",
    "\n",
    "    conn = bp.create_connection(spp_db)\n",
    "\n",
    "    get_new_data_start_time = time.time()\n",
    "        \n",
    "    for file in [f for f  in sorted(os.listdir(zip_fldr_path)) if '.xlsx' in f]:\n",
    "\n",
    "        file_year = int(file[-9:-5])\n",
    "\n",
    "        if  file_year >= min_file_year:\n",
    "            file_path = os.path.join(zip_fldr_path, file)\n",
    "            sheet_list = get_sheet_list(file_path, file_year)\n",
    "            upload_df = pd.concat(pd.read_excel(file_path, sheet_name=sheet_list), ignore_index=True)\n",
    "            upload_df.dropna(inplace=True)\n",
    "            upload_df['Delivery Date'] = pd.to_datetime(upload_df['Delivery Date'])\n",
    "            upload_df = upload_df[upload_df['Delivery Date']>max_date]\n",
    "\n",
    "            if len(upload_df)>0:\n",
    "                min_del_date = min(upload_df['Delivery Date'])\n",
    "                max_del_date = max(upload_df['Delivery Date'])\n",
    "                upload_df.rename(columns=column_dict, inplace=True)\n",
    "                upload_df.to_sql(name='ercot_hist_spp', con=conn, if_exists='append', index=False)\n",
    "\n",
    "                file_list.append(file)\n",
    "                start_date_list.append(min_del_date)\n",
    "                end_date_list.append(max_del_date)\n",
    "                time_list.append(time.time() - get_new_data_start_time)\n",
    "\n",
    "    bp.vacuum_db(spp_db)\n",
    "    conn.close()\n",
    "\n",
    "    email_dict = {'file':file_list, 'start_date': start_date_list, 'end_date':end_date_list, 'loop_duration': time_list}\n",
    "    email_df = pd.DataFrame(email_dict)\n",
    "\n",
    "except Exception as e:\n",
    "    send_email(email_subject = 'ERCOT SPP Scrape - ' + today + ' - FAILED', email_contents='An error occured during the Eroct SPP Scraping Process: /n' + logging.error(traceback.format_exc()))\n",
    "    \n",
    "else:\n",
    "    total_script_time = time.time() - script_start_time\n",
    "    total_script_time = str(datetime.timedelta(seconds=total_script_time))\n",
    "    print(total_script_time)\n",
    "    send_email(email_subject = 'ERCOT SPP Scrape - ' + today, email_contents=['Script Time (Secs): '+ str(total_script_time) + '\\n\\n', email_df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Ercot_LMP_Scrape-Z_yZNc_4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "364bf0fcf5e439b54232c65bde3d2b69a77705950e3afc3794207192ceae56ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
